# ═══════════════════════════════════════════════════════════════════════════════
# Quaero Offline Mode Configuration Example
# ═══════════════════════════════════════════════════════════════════════════════
#
# This configuration template is for OFFLINE MODE - all data stays local.
#
# WHEN TO USE OFFLINE MODE:
#   ✅ Government data (any level: local, state, federal)
#   ✅ Healthcare records (HIPAA, privacy legislation)
#   ✅ Financial information (customer data, internal financials)
#   ✅ Personal information (PII, employee records)
#   ✅ Confidential business data (trade secrets, strategic plans)
#   ✅ Any data where breach would cause legal/reputational harm
#
# SECURITY GUARANTEES:
#   ✓ All LLM processing occurs locally using llama.cpp
#   ✓ No data sent to external APIs
#   ✓ Works completely air-gapped (after initial model download)
#   ✓ Comprehensive audit trail for compliance
#   ✓ Network isolation verifiable through code review
#
# PREREQUISITES:
#   - Go 1.25 or later
#   - Model files downloaded (~5GB total)
#   - 8-16GB RAM available
#   - Multi-core CPU (8+ cores recommended)
#   - Optional: CUDA GPU for acceleration
#
# See docs/offline-mode-setup.md for complete setup instructions.
#
# ═══════════════════════════════════════════════════════════════════════════════

# ───────────────────────────────────────────────────────────────────────────────
# Server Configuration
# ───────────────────────────────────────────────────────────────────────────────

[server]
host = "localhost"
port = 8085

# Environment Variable Overrides:
#   QUAERO_SERVER_HOST=localhost
#   QUAERO_SERVER_PORT=8085

# ───────────────────────────────────────────────────────────────────────────────
# Storage Configuration
# ───────────────────────────────────────────────────────────────────────────────

[storage]
type = "sqlite"  # Currently only SQLite is supported

[storage.sqlite]
# Database file path - all data stored here
path = "./data/quaero.db"

# Enable FTS5 for full-text search
enable_fts5 = true

# Enable sqlite-vec extension for vector similarity search
enable_vector = true

# Vector dimension must match embedding model output
# nomic-embed-text-v1.5 produces 768-dimensional embeddings
embedding_dimension = 768

# SQLite performance tuning
cache_size_mb = 64        # Cache size in megabytes
wal_mode = true           # Write-Ahead Logging for better concurrency
busy_timeout_ms = 5000    # Wait up to 5 seconds if database is locked

# Environment Variable Overrides:
#   QUAERO_STORAGE_TYPE=sqlite
#   QUAERO_SQLITE_PATH=./data/quaero.db

[storage.filesystem]
# File storage paths for collected assets
images = "./data/images"
attachments = "./data/attachments"

# ───────────────────────────────────────────────────────────────────────────────
# LLM Configuration - OFFLINE MODE (SECURE)
# ───────────────────────────────────────────────────────────────────────────────

[llm]
# CRITICAL: Set mode to "offline" for local processing
# WARNING: Never use "cloud" mode with sensitive data
mode = "offline"

# Environment Variable Override:
#   QUAERO_LLM_MODE=offline

# ───────────────────────────────────────────────────────────────────────────────
# Offline LLM Configuration
# ───────────────────────────────────────────────────────────────────────────────

[llm.offline]
# Directory containing GGUF model files
# Models are loaded from this directory on startup
model_dir = "./models"

# Embedding model file (relative to model_dir)
# Recommended: nomic-embed-text-v1.5-q8.gguf (~150MB)
# Produces 768-dimensional embeddings
# Download: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF
embed_model = "nomic-embed-text-v1.5-q8.gguf"

# Chat/completion model file (relative to model_dir)
# Recommended: qwen2.5-7b-instruct-q4.gguf (~4.5GB)
# Good balance of quality and resource usage
# Download: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF
chat_model = "qwen2.5-7b-instruct-q4.gguf"

# Context window size (tokens)
# Larger = more context but slower and more RAM
# 2048: Good for most queries (~1500 words context)
# 4096: Better for complex queries (~3000 words context)
# 8192: Maximum for detailed analysis (~6000 words context)
context_size = 2048

# CPU threads for inference
# Recommendation: Number of physical cores (not hyperthreads)
# Too high: Diminishing returns, higher power usage
# Too low: Slower inference
# Typical: 4-8 for desktop, 8-16 for server
thread_count = 4

# GPU acceleration (requires CUDA)
# 0: CPU-only (works everywhere, slower)
# >0: Number of model layers to offload to GPU
# Typical values:
#   RTX 3060 (12GB): 20-25 layers
#   RTX 3090 (24GB): 35-40 layers
#   RTX 4090 (24GB): 40-45 layers
#   A100 (40GB): All layers (~60)
gpu_layers = 0

# Environment Variable Overrides:
#   QUAERO_LLM_OFFLINE_MODEL_DIR=./models
#   QUAERO_LLM_OFFLINE_EMBED_MODEL=nomic-embed-text-v1.5-q8.gguf
#   QUAERO_LLM_OFFLINE_CHAT_MODEL=qwen2.5-7b-instruct-q4.gguf
#   QUAERO_LLM_OFFLINE_CONTEXT_SIZE=2048
#   QUAERO_LLM_OFFLINE_THREAD_COUNT=4
#   QUAERO_LLM_OFFLINE_GPU_LAYERS=0

# ───────────────────────────────────────────────────────────────────────────────
# Cloud LLM Configuration (NOT USED IN OFFLINE MODE)
# ───────────────────────────────────────────────────────────────────────────────
#
# WARNING: Cloud mode sends all data to external APIs.
# ONLY use cloud mode if you:
#   • Understand the security implications
#   • Are processing personal/non-sensitive data
#   • Accept the risk of data exposure
#   • Are NOT subject to data privacy regulations
#
# See config.cloud.example.toml for cloud configuration.
#
# [llm.cloud]
# provider = "gemini"
# api_key = ""  # Set via QUAERO_LLM_CLOUD_API_KEY
# embed_model = "text-embedding-004"
# chat_model = "gemini-1.5-flash"
# max_tokens = 2048
# temperature = 0.7

# ───────────────────────────────────────────────────────────────────────────────
# LLM Audit Configuration
# ───────────────────────────────────────────────────────────────────────────────

[llm.audit]
# Enable audit logging for compliance
# Records all LLM operations (embed, chat) with metadata
# Required for regulatory compliance and incident investigation
enabled = true

# Log query text in audit records
# WARNING: May contain PII or sensitive data
# Recommendation: Keep false for privacy, enable only for debugging
log_queries = false

# Environment Variable Overrides:
#   QUAERO_LLM_AUDIT_ENABLED=true
#   QUAERO_LLM_AUDIT_LOG_QUERIES=false

# ───────────────────────────────────────────────────────────────────────────────
# Data Source Configuration
# ───────────────────────────────────────────────────────────────────────────────

[sources.confluence]
enabled = true
# Spaces to collect (empty = all accessible spaces)
spaces = []

[sources.jira]
enabled = true
# Projects to collect (empty = all accessible projects)
projects = []

[sources.github]
enabled = false
# GitHub personal access token (set via environment variable)
token = ""
# Repositories to collect (format: "owner/repo")
repos = []

# Environment Variable Overrides:
#   QUAERO_GITHUB_TOKEN=ghp_xxxxxxxxxxxx

# ───────────────────────────────────────────────────────────────────────────────
# Embeddings Configuration
# ───────────────────────────────────────────────────────────────────────────────

[embeddings]
# Enable automatic embedding generation for documents
enabled = true

# Vector dimension (must match llm.offline.embedding_dimension)
dimension = 768

# ───────────────────────────────────────────────────────────────────────────────
# Processing Configuration
# ───────────────────────────────────────────────────────────────────────────────

[processing]
# Enable scheduled background processing
# When enabled, runs on the schedule defined below
# When disabled, processing only occurs on manual trigger
enabled = false

# Cron schedule for background processing
# Format: "second minute hour day-of-month month day-of-week"
# Examples:
#   "0 0 */6 * * *"   - Every 6 hours
#   "0 0 0 * * *"     - Daily at midnight
#   "0 0 2 * * MON"   - Every Monday at 2 AM
#   "0 30 */4 * * *"  - Every 4 hours at :30
schedule = "0 0 */6 * * *"

# ───────────────────────────────────────────────────────────────────────────────
# Logging Configuration
# ───────────────────────────────────────────────────────────────────────────────

[logging]
# Log level: "debug", "info", "warn", "error"
# debug: Verbose output for troubleshooting
# info: Normal operation (recommended)
# warn: Only warnings and errors
# error: Only errors
level = "info"

# Log format: "text" or "json"
# text: Human-readable colored output
# json: Structured logs for parsing/analysis
format = "text"

# Log outputs: ["stdout", "file"]
# stdout: Console output
# file: Write to ./logs/quaero.log
output = ["stdout", "file"]

# Environment Variable Overrides:
#   QUAERO_LOG_LEVEL=info
#   QUAERO_LOG_FORMAT=text
#   QUAERO_LOG_OUTPUT=stdout,file

# ═══════════════════════════════════════════════════════════════════════════════
# End of Configuration
# ═══════════════════════════════════════════════════════════════════════════════
#
# NEXT STEPS:
#   1. Copy this file: cp config.offline.example.toml config.toml
#   2. Download models: See docs/offline-mode-setup.md
#   3. Adjust thread_count and gpu_layers for your hardware
#   4. Configure data sources (Confluence, Jira, GitHub)
#   5. Run: ./quaero serve --config config.toml
#
# VERIFICATION:
#   • Startup banner shows "✓ OFFLINE (local processing)"
#   • No "WARNING: Cloud mode active" message
#   • Logs show "Loading embedding model" and "Loading chat model"
#   • Check audit log: SELECT * FROM audit_log WHERE mode = 'offline';
#
# SUPPORT:
#   Documentation: docs/offline-mode-setup.md
#   Troubleshooting: docs/offline-mode-setup.md#troubleshooting
#   GitHub Issues: https://github.com/ternarybob/quaero/issues
#
# ═══════════════════════════════════════════════════════════════════════════════
