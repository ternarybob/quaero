# Quaero Production Configuration (Docker)
# ==========================================
#
# ⚠️  DEPRECATED: This configuration file is deprecated and no longer supported.
# ⚠️  Offline LLM mode and embeddings have been removed from Quaero.
# ⚠️  Use cloud-based Google Gemini API instead (see deployments/local/quaero.toml).
#
# This is a full example configuration for production deployment with Docker.
# Copy this file to config.toml and customize for your environment.
#
# Key Production Settings:
# - environment = "production" (rejects test URLs like localhost)
# - Proper resource limits and timeouts
#
# Configuration Load Order (priority from lowest to highest):
#   1. Defaults (hardcoded in application)
#   2. TOML file (this file)
#   3. Environment variables (QUAERO_*)
#   4. Command line flags (--port, --host, --config)

# =============================================================================
# Environment Configuration
# =============================================================================
# Controls environment mode: "development" or "production"
# Production mode enforces stricter validation:
#  - Rejects test URLs (localhost, 127.0.0.1, ::1, port 3333)
#  - Ensures seed URLs point to real production services
# Development mode allows test URLs for local testing
# Env vars: QUAERO_ENV, GO_ENV (highest priority)

environment = "production"  # IMPORTANT: Production mode rejects test URLs

# =============================================================================
# Server Configuration
# =============================================================================
[server]
port = 8080
host = "0.0.0.0"  # Listen on all interfaces in Docker

# =============================================================================
# Data Sources Configuration (DEPRECATED)
# =============================================================================
# NOTE: Source-specific configuration sections have been removed.
# Data collection is now managed exclusively through Crawler Jobs.
#
# To configure data sources for production:
# 1. Define crawler job definitions in the job-definitions/ directory
# 2. Jobs specify seed URLs, URL patterns, and crawl rules
# 3. Enable/disable jobs via the jobs configuration section below
# 4. Mount job-definitions/ directory in Docker volume
#
# The generic ChromeDP-based crawler replaces direct API integrations.
# See job-definitions/ directory for examples of Jira/Confluence/GitHub crawlers.
#
# Legacy sections removed: [sources.jira], [sources.confluence], [sources.github]

# =============================================================================
# Storage Configuration
# =============================================================================
[storage.badger]
path = "/data"  # Docker volume mount point

[storage.filesystem]
images = "/data/images"
attachments = "/data/attachments"

# =============================================================================
# Gemini Configuration (Google ADK - Unified for Agents and Chat)
# =============================================================================
# ⚠️  DEPRECATED NOTICE: Offline LLM mode has been removed.
# Use Google Gemini API for all AI operations (agents and chat).
#
# [gemini]
# google_api_key = "YOUR_GOOGLE_GEMINI_API_KEY"  # Required for all AI operations
# agent_model = "gemini-2.0-flash"                # Model for agent operations
# chat_model = "gemini-2.0-flash"                 # Model for chat operations
# max_turns = 10                                   # Maximum agent conversation turns
# timeout = "5m"                                   # Operation timeout
# temperature = 0.7                                # Chat completion temperature

# =============================================================================
# DEPRECATED: The following sections are no longer supported
# =============================================================================
# - [llm] and [llm.offline]: Offline LLM mode removed
# - [llm.audit]: Audit logging removed with offline mode
# - [rag]: RAG configuration moved to application code
# - [embeddings]: Embeddings functionality removed (using FTS5 search only)

# =============================================================================
# Processing Configuration
# =============================================================================
[processing]
enabled = true
schedule = "0 0 */6 * * *"  # Every 6 hours
limit = 5000                 # Max documents per run

# =============================================================================
# Logging Configuration
# =============================================================================
[logging]
level = "info"                     # Production log level (debug|info|warn|error)
format = "json"                    # JSON for log aggregation
output = ["stdout", "file"]        # Log to stdout and file
client_debug = false               # Disable client debug in production

# =============================================================================
# Default Jobs Configuration
# =============================================================================
# System-managed scheduled tasks
# Minimum interval is 5 minutes
# Schedule uses cron format: minute hour day month weekday

[jobs.crawl_and_collect]
enabled = true
schedule = "*/5 * * * *"  # Every 5 minutes
description = "Crawl and collect website data, store as markdown"

[jobs.scan_and_summarize]
enabled = true
schedule = "*/10 * * * *"  # Every 10 minutes
description = "Scan markdown documents and generate summaries with metadata"

# =============================================================================
# Crawler Configuration (Firecrawl-style HTML Scraping)
# =============================================================================
# Production-tuned crawler settings
[crawler]
user_agent = "Quaero/1.0 (Web Crawler; +https://your-domain.com/bot)"
user_agent_rotation = true
max_concurrency = 5                # Production concurrency
request_delay = "2s"               # Respectful rate limiting
random_delay = "1s"                # Random jitter
request_timeout = "60s"            # Production timeout
max_body_size = 20971520           # 20MB
max_depth = 5                      # Crawl depth
follow_robots_txt = true           # Respect robots.txt
output_format = "markdown"         # Markdown for LLM consumption
only_main_content = true           # Extract main content only
include_links = true               # Discover links
include_metadata = true            # Extract metadata
use_html_seeds = true              # Use HTML page URLs (not REST APIs)
allowed_content_types = ["text/html", "application/json"]  # Allowed content types
enable_empty_output_fallback = true  # Fallback for empty conversions
