// Example: How to integrate OfflineLLMService into Quaero
// This file shows the integration pattern - DO NOT BUILD THIS FILE

package main

import (
	"context"
	"fmt"
	"log"

	"github.com/ternarybob/arbor"
	"github.com/ternarybob/quaero/internal/common"
	"github.com/ternarybob/quaero/internal/interfaces"
	"github.com/ternarybob/quaero/internal/services/llm/offline"
)

// Example 1: Initialize from config
func initializeLLMServiceFromConfig(config *common.Config, logger arbor.ILogger) (interfaces.LLMService, error) {
	if config.LLM.Mode != "offline" {
		return nil, fmt.Errorf("offline mode not configured")
	}

	service, err := offline.NewOfflineLLMService(
		config.LLM.Offline.ModelDir,
		config.LLM.Offline.EmbedModel,
		config.LLM.Offline.ChatModel,
		config.LLM.Offline.ContextSize,
		config.LLM.Offline.ThreadCount,
		config.LLM.Offline.GPULayers,
		logger,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to initialize offline LLM: %w", err)
	}

	// Verify service is healthy
	ctx := context.Background()
	if err := service.HealthCheck(ctx); err != nil {
		service.Close()
		return nil, fmt.Errorf("LLM health check failed: %w", err)
	}

	logger.Info().
		Str("mode", string(service.GetMode())).
		Str("embed_model", config.LLM.Offline.EmbedModel).
		Str("chat_model", config.LLM.Offline.ChatModel).
		Msg("LLM service initialized")

	return service, nil
}

// Example 2: Use in embedding service
func generateDocumentEmbedding(llmService interfaces.LLMService, text string) ([]float32, error) {
	ctx := context.Background()

	// Generate embedding
	embedding, err := llmService.Embed(ctx, text)
	if err != nil {
		return nil, fmt.Errorf("embedding generation failed: %w", err)
	}

	if len(embedding) != 768 {
		return nil, fmt.Errorf("expected 768-dim embedding, got %d", len(embedding))
	}

	return embedding, nil
}

// Example 3: Use in search/RAG pipeline
func searchWithRAG(llmService interfaces.LLMService, query string, contextDocs []string) (string, error) {
	ctx := context.Background()

	// Build RAG prompt
	systemPrompt := "You are a helpful assistant. Answer based on the provided context."

	contextText := "Context:\n"
	for i, doc := range contextDocs {
		contextText += fmt.Sprintf("%d. %s\n", i+1, doc)
	}

	userPrompt := contextText + "\nQuestion: " + query

	messages := []interfaces.Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userPrompt},
	}

	// Generate response
	response, err := llmService.Chat(ctx, messages)
	if err != nil {
		return "", fmt.Errorf("chat completion failed: %w", err)
	}

	return response, nil
}

// Example 4: Startup sequence in main.go
func exampleStartupSequence() {
	// 1. Load config
	config, err := common.LoadFromFile("config.toml")
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	// 2. Initialize logger
	logger := arbor.NewLogger()

	// 3. Initialize LLM service based on mode
	var llmService interfaces.LLMService

	if config.LLM.Mode == "offline" {
		llmService, err = offline.NewOfflineLLMService(
			config.LLM.Offline.ModelDir,
			config.LLM.Offline.EmbedModel,
			config.LLM.Offline.ChatModel,
			config.LLM.Offline.ContextSize,
			config.LLM.Offline.ThreadCount,
			config.LLM.Offline.GPULayers,
			logger,
		)
		if err != nil {
			log.Fatalf("Failed to initialize offline LLM: %v", err)
		}
	} else {
		// Initialize cloud LLM service (not implemented yet)
		log.Fatal("Cloud mode not yet implemented")
	}

	defer llmService.Close()

	// 4. Verify LLM is healthy
	ctx := context.Background()
	if err := llmService.HealthCheck(ctx); err != nil {
		log.Fatalf("LLM health check failed: %v", err)
	}

	logger.Info().
		Str("mode", string(llmService.GetMode())).
		Msg("LLM service ready")

	// 5. Continue with rest of application startup...
}

// Example 5: Dependency injection pattern
type App struct {
	config     *common.Config
	logger     arbor.ILogger
	llmService interfaces.LLMService
	// ... other services
}

func NewApp(configPath string) (*App, error) {
	// Load config
	config, err := common.LoadFromFile(configPath)
	if err != nil {
		return nil, err
	}

	// Initialize logger
	logger := arbor.NewLogger()

	// Initialize LLM service
	llmService, err := offline.NewOfflineLLMService(
		config.LLM.Offline.ModelDir,
		config.LLM.Offline.EmbedModel,
		config.LLM.Offline.ChatModel,
		config.LLM.Offline.ContextSize,
		config.LLM.Offline.ThreadCount,
		config.LLM.Offline.GPULayers,
		logger,
	)
	if err != nil {
		return nil, err
	}

	return &App{
		config:     config,
		logger:     logger,
		llmService: llmService,
	}, nil
}

func (a *App) Close() error {
	return a.llmService.Close()
}
