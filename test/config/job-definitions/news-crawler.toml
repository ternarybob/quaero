# Example Crawler Job Definition
# This file demonstrates how to define a custom crawler job in TOML format
# Place .toml or .json files in the job-definitions directory to auto-load at startup

id = "news-crawler"
name = "News Crawler"
type = "crawler"
job_type = "user"
description = "Crawler job that crawls a news website and filters for specific content"

# Initial URLs to start crawling from
start_urls = ["https://www.abc.net.au/news"]

schedule = ""

# Maximum execution time (e.g., "30m", "1h", "2h30m")
timeout = "30m"

# Whether this job is enabled
enabled = true

# Whether to start automatically on service startup
auto_start = false

# URL patterns to include (regex patterns)
include_patterns = ["abc\\.net\\.au/news"]

# URL patterns to exclude (regex patterns)
exclude_patterns = ["/video/", "/audio/", "/live-blog/"]

# Crawler behavior
max_depth = 3       # Maximum depth to follow links
max_pages = 50      # Maximum number of pages to crawl
concurrency = 5     # Number of concurrent workers
follow_links = true # Whether to follow discovered links
