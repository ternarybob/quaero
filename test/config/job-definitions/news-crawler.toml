# Example Crawler Job Definition
# This file demonstrates how to define a custom crawler job in TOML format
# Place .toml or .json files in the job-definitions directory to auto-load at startup

id = "news-crawler"
name = "News Crawler"
description = "Crawler job that crawls a news website and filters for specific content"

schedule = ""

# Maximum execution time (e.g., "30m", "1h", "2h30m")
timeout = "30m"

# Whether this job is enabled
enabled = true

# Whether to start automatically on service startup
auto_start = false

# Extension matching: when true, this job can be matched by Chrome extension
extension = true

# URL patterns for automatic job matching (wildcards supported)
# Quick crawl will use this job definition when the URL matches any pattern
url_patterns = ["*.abc.net.au/*", "stockhead.com.au/*"]

[step.crawl_news]
type = "crawler"
description = "Crawl ABC News and extract article content"
on_error = "fail"
# Initial URLs to start crawling from
start_urls = ["https://www.abc.net.au/news"]
# URL patterns to include (regex patterns)
include_patterns = ["abc\\.net\\.au/news"]
# URL patterns to exclude (regex patterns)
exclude_patterns = ["/video/", "/audio/", "/live-blog/"]
# Crawler behavior
max_depth = 2       # Maximum depth to follow links
max_pages = 10      # Maximum number of pages to crawl (GLOBAL limit)
concurrency = 5     # Number of concurrent workers
follow_links = true # Whether to follow discovered links
